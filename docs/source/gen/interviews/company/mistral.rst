#########################################################
Mistral
#########################################################
*********************************************************
Screen / ML quiz
*********************************************************
Transformer internals, attention, positional encodings, scaling laws, optimization (Adam, learning rate schedules), training instabilities, evaluation, and fine‑tuning methods for LLMs.

*********************************************************
Coding + ML implementation round
*********************************************************
Implementing core ML algorithms from scratch (e.g., K‑means, simple training loops) in Python or PyTorch, with attention to correctness and performance.

*********************************************************
Deep‑dive research / systems rounds
*********************************************************
Questions about training large models on multi‑GPU clusters (NCCL, OOM failures, parallelism strategies).
Discussion of past research, papers you have written/read, and how you would improve Mistral‑style models.
​
