##########################################################################################
Reading List
##########################################################################################

******************************************************************************************
Trendy
******************************************************************************************
- https://www.emergentmind.com/

******************************************************************************************
Ilya's Paper List
******************************************************************************************
* `arc.net/folder <https://arc.net/folder/D0472A20-9C20-4D3F-B145-D2865C0A9FEE>`_

Fundamentals
------------------------------------------------------------------------------------------
* The Annotated Transformer
* The First Law of Complexodynamics
* The Unreasonable Effectiveness of RNNs
* Understanding LSTM Networks
* Recurrent Neural Network Regularization
* Keeping Neural Networks Simple by Minimizing the Description Length of the Weights
* Pointer Networks
* ImageNet Classification with Deep CNNs
* Order Matters: Sequence to Sequence for Sets
* GPipe: Efficient Training of Giant Neural Networks
* Deep Residual Learning for Image Recognition
* Multi-Scale Context Aggregation by Dilated Convolutions
* Neural Quantum Chemistry
* Attention Is All You Need
* Neural Machine Translation by Jointly Learning to Align and Translate
* Identity Mappings in Deep Residual Networks
* A Simple NN Module for Relational Reasoning
* Variational Lossy Autoencoder
* Relational RNNs
* Quantifying the Rise and Fall of Complexity in Closed Systems
* Neural Turing Machines
* Deep Speech 2: End-to-End Speech Recognition in English and Mandarin
* Scaling Laws for Neural LMs (arxiv.org)
* A Tutorial Introduction to the Minimum Description Length Principle (arxiv.org)
* Machine Super Intelligence Dissertation (vetta.org)
* PAGE 434 onwards: Komogrov Complexity (lirmm.fr)
* CS231n Convolutional Neural Networks for Visual Recognition (cs231n.github.io)

Foundations and Trends® in Machine Learning
------------------------------------------------------------------------------------------
`https://www.nowpublishers.com/MAL <https://www.nowpublishers.com/MAL>`_

Additional Resources
------------------------------------------------------------------------------------------
* [GPT-1](https://lnkd.in/gJ5Pe3HG)
* [GPT-2](https://lnkd.in/gatQi8Ud)
* [GPT-3](https://lnkd.in/g43GzYfZ)
* [GPT-4](https://lnkd.in/ga_xEpEj)
* [Llama-2](https://lnkd.in/gutaGW8h)
* [Tools](https://lnkd.in/gqJ3aXpS)
* [Gemini-Pro-1.5](https://lnkd.in/gbDcYp89)
* [Agentic Patterns Series](https://lnkd.in/gphZ6Y5s)

******************************************************************************************
University courses
******************************************************************************************
* [stanford.edu] `cs224n Natural Language Processing <https://web.stanford.edu/class/cs224n/>`_
* [stanford.edu] `cs25 Transformers United <https://web.stanford.edu/class/cs25/>`_
* [stanford.edu] `cs330 Deep Multi-task and Meta Learning <https://cs330.stanford.edu/>`_
* [stanford.edu] `cs224w Machine Learning with Graphs <https://web.stanford.edu/class/cs224w/>`_
* [stanford.edu] `cs231n Convolutional Neural Networks for Visual Recognition <https://cs231n.github.io/>`_
* [stanford.edu] `Deep generative models <https://deepgenerativemodels.github.io/>`_
* [stanford.edu] `cs229s Systems for Machine Learning <https://cs229s.stanford.edu/fall2023/>`_
* [stanford.edu] `Stanford MLSys Seminar <https://mlsys.stanford.edu/>`_
* [stanford.edu] `cs246 Mining Massive Dataset <https://web.stanford.edu/class/cs246/>`_
* [stanford.edu] `Math 104 Applied Matrix Theory <https://candes.su.domains/teaching/math104/>`_
* [cs.cmu.edu] `Large Language Model Systems <https://llmsystem.github.io/llmsystem2024spring/>`_
* [cs.cmu.edu] `15-496/15-859X: Computer Science Theory for the Information Age, Spring 2012 <https://www.cs.cmu.edu/~venkatg/teaching/CStheory-infoage/>`_
* `aman.ai <https://aman.ai/>`_

******************************************************************************************
Search and Recommender Systems
******************************************************************************************
.. csv-table:: 
	:header: "Tag", "Title"
	:align: center
	
		Survey; CTR, Deep Learning for Click-Through Rate Estimation
		Survey; Embedding, Embedding in Recommender Systems: A Survey
		Survey; Ranking, A Survey on Accuracy-Oriented Neural Recommendation: From Collaborative Filtering to Information-Rich Recommendation 
		Survey; Retrieval, A Comprehensive Survey on Retrieval Methods in Recommender Systems
		Survey; Objective, Self-Supervised Learning for Recommender Systems: A Survey
		Method; CTR, Factorization Machines
		Method; CTR, Wide & Deep Learning for Recommender Systems
		Method; CTR, DeepFM: A Factorization-Machine based Neural Network for CTR Predictio
		Method; CTR, xDeepFM: Combining Explicit and Implicit Feature Interactions for Recommender Systems
		Method; CTR, Deep & Cross Network for Ad Click Predictions
		Method; CTR, DCN V2: Improved Deep & Cross Network and Practical Lessons for Web-scale Learning to Rank Systems
		Method; CTR, Deep Interest Network for Click-Through Rate Prediction
		Method; Feedback Delay, Addressing Delayed Feedback for Continuous Training with Neural Networks in CTR prediction

******************************************************************************************
LLMs
******************************************************************************************
.. note::
	* [Practical] `The Large Language Model Playbook <https://cyrilzakka.github.io/llm-playbook/index.html>`_
	* [Practice] `masteringllm.com <https://www.masteringllm.com/#/home>`_
	* [Sebastian Rachka] `Understanding Large Language Models <https://magazine.sebastianraschka.com/p/understanding-large-language-models>`_
	* [pair.withgoogle.com] `Can Large Language Models Explain Their Internal Mechanisms? <https://pair.withgoogle.com/explorables/patchscopes/>`_
	* [MIT] `Self-Supervised Learning and Foundation Models <https://www.futureofai.mit.edu/>`_
	* `Transformer Math 101 <https://blog.eleuther.ai/transformer-math/>`_
	* `Large Language Models: Scaling Laws and Emergent Properties <https://cthiriet.com/articles/scaling-laws>`_
	* [Github] `LLM Course <https://github.com/mlabonne/llm-course>`_

.. important::
	* `Generative Agents: Interactive Simulacra of Human Behavior <https://arxiv.org/pdf/2304.03442.pdf>`_
	* `Locating and Editing Factual Associations in GPT <https://arxiv.org/pdf/2202.05262.pdf>`_
	* `Jarvis/HuggingGPT <https://github.com/microsoft/JARVIS>`_
	* `Sparks of Artificial General Intelligence <https://arxiv.org/pdf/2303.12712.pdf>`_
	* `Reflexion: an autonomous agent with dynamic memory and self-reflection <https://arxiv.org/pdf/2303.11366.pdf>`_
	* `Hyena Hierarchy: Towards Larger Convolutional Language Models <https://arxiv.org/pdf/2302.10866.pdf>`_
	* `Scaling Transformer to 1M tokens and beyond with RMT <https://arxiv.org/pdf/2304.11062.pdf>`_
	* `AI/ML/LLM/Transformer Models Timeline and List <https://ai.v-gar.de/ml/transformer/timeline/>`_
	* `Think Before You Act: Unified Policy for Interleaving Language Reasoning with Actions <https://arxiv.org/pdf/2304.11063.pdf>`_
	* `Generative Verifiers: Reward Modeling as Next-Token Prediction <https://arxiv.org/abs/2408.15240>`_

******************************************************************************************
Applied LLMs
******************************************************************************************
.. note::
	* [Blog] `eugeneyan.com <https://eugeneyan.com/>`_
	* [Blog] `sh-reya.com <https://www.sh-reya.com/blog>`_
	* [Blog] `hamel.dev <https://hamel.dev/>`_
	* [Github] `LLM4Rec: Collection of papers <https://github.com/WLiK/LLM4Rec-Awesome-Papers>`_
	* [Github] Large Language Models for Generative Information Extraction: `Awesome-LLM4IE-Papers <https://github.com/quqxui/Awesome-LLM4IE-Papers>`_
	* [Github] Large Language Models Meet NLP: `Awesome-LLM-for-NLP <https://github.com/LightChen233/Awesome-LLM-for-NLP>`_
	* [Github] Knowledge graphs (KGs) and large language models (LLMs): `KG-LLM-Papers <https://github.com/zjukg/KG-LLM-Papers>`_
	* [Harvard] CS50 Tech Talk: `GPT-4 - How does it work, and how do I build apps with it? <https://www.youtube.com/watch?v=vw-KWfKwvTQ>`_
	* [Stanford] `HELM - Holistic Evaluation of Language Models <https://crfm.stanford.edu/helm/latest/>`_

.. important::
	* `Freepik - A New Search for the New World <https://www.freepik.com/blog/new-search-new-world/>`_
	* `Replacing my best friends with an LLM <https://www.izzy.co/blogs/robo-boys.html>`_
	* `Become a 1000x engineer or die tryin <https://kadekillary.work/posts/1000x-eng/>`_
	* `Man and machine: GPT for second brains <https://reasonabledeviations.com/2023/02/05/gpt-for-second-brain/>`_
	* `Learn Prompting <https://learnprompting.org/>`_
	* `Prompt Engineering vs. Blind Prompting <https://mitchellh.com/writing/prompt-engineering-vs-blind-prompting>`_
	* `An example of LLM prompting for programming <https://martinfowler.com/articles/2023-chatgpt-xu-hao.html>`_
	* `Chat with any PDF <https://www.chatpdf.com/>`_
	* `AI prompt-to-storyboard videos w/ GPT, Coqui voices, StabilityAI images <https://meyer.id/>`_
	* `ChatGPT for your site <https://letterdrop.com/chatgpt?ref=hn>`_
	* `Web LLM runs the vicuna-7b Large Language Model entirely in your browser <https://simonwillison.net/2023/Apr/16/web-llm/>`_
	* [Paper] `AI Agents That Matter <https://arxiv.org/pdf/2407.01502>`_
	* `In Defense of RAG in the Era of Long-Context Language Models <https://arxiv.org/pdf/2409.01666>`_
	* `Retrieval Augmented Generation or Long-Context LLMs? A Comprehensive Study and Hybrid Approach <https://arxiv.org/abs/2407.16833>`_
	* `Inductive or Deductive? Rethinking the Fundamental Reasoning Abilities of LLMs <https://arxiv.org/abs/2408.00114>`_
	* `Speculative RAG: Enhancing Retrieval Augmented Generation through Drafting <https://arxiv.org/abs/2407.08223>`_
	* `Graph Retrieval-Augmented Generation: A Survey <https://arxiv.org/abs/2408.08921>`_
	* `Retrieval-Augmented Generation with Knowledge Graphs for Customer Service Question Answering <https://arxiv.org/abs/2404.17723>`_

******************************************************************************************
Must Read Papers
******************************************************************************************
.. csv-table:: 
	:header: "Tag", "Title"
	:align: center
	
		Attention,MHA: Attention Is All You Need
		Attention,MQA: Fast Transformer Decoding: One Write-Head is All You Need
		Attention,GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints
		Attention,Attention is Turing Complete
		Decoding,Fast Inference from Transformers via Speculative Decoding
		Activation,GLU Variants Improve Transformer
		Norm,Layer Normalization
		Norm,Root Mean Square Layer Normalization
		PE,RoFormer: Enhanced Transformer with Rotary Position Embedding
		MLM, BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
		MLM, RoBERTa: A Robustly Optimized BERT Pretraining Approach
		MLM, TwinBERT: Distilling Knowledge to Twin-Structured BERT Models for Efficient Retrieval		
		MLM, Finetuned Language Models Are Zero-Shot Learners
		RTD, Electra: Pre-training Text Encoders as Discriminators Rather Than Generators
		CLM, GPT2: Language Models are Unsupervised Multitask Learners
		CLM, GPT3: Language Models are Few-Shot Learners		
		CLM, LLaMA: Open and Efficient Foundation Language Models
		CLM, LLaMA 2: Open Foundation and Fine-Tuned Chat Models
		CLM, LLaMA 3: The Llama 3 Herd of Models
		MoE, Mixtral: Mixtral of Experts
		MoE, DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning		
		PLM, XLNet: Generalized Autoregressive Pretraining for Language Understanding
		GLM, GLM: General Language Model Pretraining with Autoregressive Blank Infilling
		MoE,Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity		
		MoE, OLMoE: Open Mixture-of-Experts Language Models
		Seq2Seq, BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation Translation and Comprehension
		Seq2Seq, T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer
		Multilingual, XLM: Cross-lingual Language Model Pretraining
		Multilingual, XLM-R: Unsupervised Cross-lingual Representation Learning at Scale
		Multilingual, mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer
		Generalisation,Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets
		Scaling, Scaling Laws for Neural Language Models
		Scaling, Scaling Laws for Autoregressive Generative Modeling
		Scaling, Scaling Laws for Data Filtering -- Data Curation cannot be Compute Agnostic
		Contrastive, E5: Text Embeddings by Weakly-Supervised Contrastive Pre-training
		Contrastive, Unsupervised Dense Information Retrieval with Contrastive Learning
		IR, Dense Passage Retrieval for Open-Domain Question Answering
		IE:NER, UniversalNER: Targeted Distillation from Large Language Models for Open Named Entity Recognition
		LLM, Aligning language models to follow instructions
		LLM, Scaling Instruction-Finetuned Language Models
		LLM, InstructGpt: Training language models to follow instructions with human feedback
		LLM, Injecting New Knowledge into Large Language Models via Supervised Fine-Tuning
		LLM, The Instruction Hierarchy: Training LLMs to Prioritize Privileged Instructions
		LLM, Self-instruct: Aligning language model with self generated instructions
		LLM, PPO: Proximal Policy Optimization Algorithms
		LLM, SFT+RLHF: Learning to summarize from human feedback
		LLM, Reflexion: Language Agents with Verbal Reinforcement Learning
		LLM, RLCD: Reinforcement Learning from Contrastive Distillation for Language Model Alignment
		LLM, On-Policy Distillation of Language Models: Learning from Self-Generated Mistakes
		LLM, DPO: Direct Preference Optimization: Your Language Model is Secretly a Reward Model
		LLM, Understanding Reference Policies in Direct Preference Optimization
		LLM, D2PO: Discriminator-Guided DPO with Response Evaluation Models
		LLM, Aligning with Human Judgement: The Role of Pairwise Preference in Large Language Model Evaluators
		LLM, RAG: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks
		LLM, Large Language Models Are Latent Variable Models
		LLM, DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models
		Quant, LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale
		Quant, LoRA Low-Rank Adaptation of Large Language Models
		Quant, QLORA: Efficient Finetuning of Quantized LLMs
		Quant, SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models
		DiT, Scalable Diffusion Models with Transformers
		DiT, Scaling Rectified Flow Transformers for High-Resolution Image Synthesis
		ViT, Patch n' Pack: NaViT - a Vision Transformer for any Aspect Ratio and Resolution		
		ViT, Long Context Transfer from Language to Vision
		Multimodal, Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model
		Eval, HELM: Holistic Evaluation of Language Models
		Eval, MMLU: Measuring Massive Multitask Language Understanding
		Eval, MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI
		Hallucination, SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models
		Hallucination, G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment
		Hallucination, To Believe or Not to Believe Your LLM
		Representation, Scaling and evaluating sparse autoencoders
		Representation, Probabilistic Topic Modelling with Transformer Representations
		Representation, Matryoshka Representation Learning
		Representation, Not All Language Model Features Are Linear
		Context: Full, FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness
		Context: Full, FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning
		Context: Full, FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision
		Context: Full, Effective Long-Context Scaling of Foundation Models
		Context: Sparse, Longformer: The Long-Document Transformer
		Context: Sparse, Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context
		Context, Ring Attention with Blockwise Transformers for Near-Infinite Context
		Context, Lost in the Middle: How Language Models Use Long Contexts
		Long Context, ChatQA 2: Bridging the Gap to Proprietary LLMs in Long Context and RAG Capabilities
		Long Context, LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models
		Long Context, YaRN: Efficient Context Window Extension of Large Language Models
		Long Context, Data Engineering for Scaling Language Models to 128K Context
		Long Context, Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention				
		Long Context, LongAlign: A Recipe for Long Context Alignment of Large Language Models
		Long Context, Chain of Agents: Large Language Models Collaborating on Long-Context Tasks
		Memory, MemoryBank: Enhancing Large Language Models with Long-Term Memory
		Memory, Augmenting Language Models with Long-Term Memory
		Memory, Recurrent Memory Transformer
		Memory, Scaling Transformer to 1M tokens and beyond with RMT
		Memory, Beyond Attention: Breaking the Limits of Transformer Context Length with Recurrent Memory
		KG, Language Models as Knowledge Bases?
		KG, Language Models are Open Knowledge Graphs
		KG, Unifying Large Language Models and Knowledge Graphs: A Roadmap
		KG, QA-GNN: Reasoning with Language Models and Knowledge Graphs for Question Answering
		KG, SimKGC: Simple Contrastive Knowledge Graph Completion with Pre-trained Language Models
		KG, Give Us the Facts: Enhancing Large Language Models with Knowledge Graphs for Fact-aware Language Modeling
		KG, Knowledge Graph Large Language Model (KG-LLM) for Link Prediction

******************************************************************************************
Math
******************************************************************************************
* [github.com] `rangaeeeee/books-mir-mathematics <https://github.com/rangaeeeee/books-mir-mathematics/>`_
* [3Blue1Brown] `Essence of linear algebra <https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab>`_
* [MIT] `18.065 - Matrix Methods for Data Analysis <https://www.youtube.com/playlist?list=PLUl4u3cNGP63oMNUHXqIUcrkS2PivhN3k>`_
* [3Blue1Brown] `Essence of calculus <https://www.youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr>`_
* [cs.princeton.edu] `High Dimensional Geometry, Curse of Dimensionality, Dimension Reduction <https://www.cs.princeton.edu/courses/archive/fall16/cos521/Lectures/lec9.pdf>`_
* [Khan Academy] `Multivariable calculus <https://www.khanacademy.org/math/multivariable-calculus>`_
* [University of Victoria] `MATH200: Calculus III: Multivariable Calculus <https://www.youtube.com/playlist?list=PLHXZ9OQGMqxc_CvEy7xBKRQr6I214QJcd>`_
* [MIT] `RES.6-012 Introduction to Probability <https://www.youtube.com/playlist?list=PLUl4u3cNGP60hI9ATjSFgLZpbNJ7myAg6>`_
* [CMU] `36-705 - Intermediate Statistics <https://www.youtube.com/playlist?list=PLt2Pd5kunvJ6-wpJG9hlWlk47c76bm9L6>`_
* [statisticsmatt] `Introduction to Mathematical Statistics with Illustrations using R <https://www.youtube.com/playlist?list=PLmM_3MA2HWpan-KlYp-QCbPHxMj5FK0TB>`_
* [phys.org] `Mathematician uncovers methods to shrink sampling errors in large-dimensional data sets <https://phys.org/news/2023-03-mathematician-uncovers-methods-sampling-errors.html>`_
* [SO] `Pointwise vs. Uniform Convergence <https://math.stackexchange.com/questions/597765/pointwise-vs-uniform-convergence#915867>`_
* [math.cornell.edu] `Linear ODE <https://e.math.cornell.edu/people/belk/writing/>`_

******************************************************************************************
ML Theory
******************************************************************************************
* [Goodfellow] `Deep Learning <https://www.deeplearningbook.org/>`_
* [Dong] `Deep Reinforcement Learning <https://deepreinforcementlearningbook.org/>`_
* [Roberts] `The Principles of Deep Learning Theory <https://arxiv.org/abs/2106.10165>`_
* [Kevin Murphy] `Probabilistic Machine Learning book1 <https://probml.github.io/pml-book/book1.html>`_
* [Kevin Murphy] `Probabilistic Machine Learning book2 <https://probml.github.io/pml-book/book2.html>`_
* [Bronstein,Bruna,Cohen,Veickovic][2021] `Geometric Deep Learning <https://geometricdeeplearning.com/>`_
* [Shwartz David] `Understanding Machine Learning - From Theory to Algorithms <https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf>`_
* [Mohri] `Foundations of Machine Learning <https://cs.nyu.edu/~mohri/mlbook/>`_
* [CMU] `11-785 Deep Learning <https://www.youtube.com/playlist?list=PLp-0K3kfddPxRmjgjm0P1WT6H-gTqE8j9>`_
* [MIT] `Statistical Learning Theory and Applications <https://cbmm.mit.edu/lh-9-520/syllabus>`_
* [GPSS] `Gaussian Process Summer School <https://gpss.cc/gpss23/program>`_
* [NUS] `Graph Machine Learning course, 2023 <https://github.com/xbresson/GML2023>`_
* `Yet Another Derivation of Backpropagation in Matrix Form <https://sudeepraja.github.io/BackpropAdjoints/>`_
* `Gradients are Not All You Need <https://arxiv.org/pdf/2111.05803.pdf>`_
* `The Decade of Deep Learning <https://bmk.sh/2019/12/31/The-Decade-of-Deep-Learning/>`_
* `Long-Tailed Learning Requires Feature Learning <https://openreview.net/pdf?id=S-h1oFv-mq>`_
* `A Survey on Deep Graph Generation: Methods and Applications <https://proceedings.mlr.press/v198/zhu22a.html>`_

******************************************************************************************
ML Practical
******************************************************************************************

* [Andrej Karpathy] `Neural Networks: Zero to Hero <https://karpathy.ai/zero-to-hero.html>`_
* `pytorch-internals <http://blog.ezyang.com/2019/05/pytorch-internals/>`_
* https://forums.fast.ai/t/diving-deep-into-pytorch/39470
* [Stevens] `Deep Learning with PyTorch <https://www.manning.com/books/deep-learning-with-pytorch>`_
* [Geron] `Hands-on Machine Learning <https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/>`_
* [Howard] `Deep Learning for Coders with fastai and PyTorch <https://course.fast.ai/Resources/book.html>`_
* [Zheng Casari] Feature Engineering for Machine Learning
* [NYU] `Deep Learning (Spring 2020) <https://atcold.github.io/pytorch-Deep-Learning/>`_
* [CMU] `Dive into Deep Learning <https://d2l.ai/index.html>`_
* [MIT] `6.S965 TinyML and Efficient Deep Learning <https://efficientml.ai/>`_
* [Microsoft Research] `LMOps <https://github.com/microsoft/LMOps>`_
* `Data Centric AI Cource <https://github.com/dcai-course/dcai-course>`_

******************************************************************************************
ML Design General Principle
******************************************************************************************

* [Andrew Ng] `Machine Learning Yearning <https://www.mlyearning.org/>`_
* [Chip Huyen] Designing Machine Learning Systems
* [Burkov] Machine Learning Engineering
* [Jeff Smith] Machine Learning Systems
* [Lakshmanan] Machine Learning Design Patterns
* [UCB] System Design for Large Scale Machine Learning

******************************************************************************************
ML Math
******************************************************************************************

* [Gutmann] Pen and Paper Exercise in ML
* `Steve Brunton Playlist <https://www.youtube.com/@Eigensteve/playlists>`_
* `Matrix Calculus <https://www.matrixcalculus.org/>`_

******************************************************************************************
ML Algorithms
******************************************************************************************

* [Naumann] The Art of Differentiating Computer Programs

******************************************************************************************
ML Related Theory
******************************************************************************************
* [nowpublishers.com] `Foundations and Trends® in Machine Learning <https://www.nowpublishers.com/MAL>`_
* [MacKay] Information Throry Inference and Learning Algorithms
* [Brunton Kutz] Data Driven Science and Engineering
* [CUP] Probabilistic Numerics
* [Easley Kleinberg] Networks Crowds and Markets - Reasoning About a Highly Connected World
* `Beyond Euclid: An Illustrated Guide to Modern Machine Learning with Geometric, Topological, and Algebraic Structures <https://www.arxiv.org/abs/2407.09468>`_

******************************************************************************************
Applied ML
******************************************************************************************

* [Liu] Learning to Rank for Information Retrieval
* [MSR] A Short Introduction to Learning to Rank
* [MSR] LambdaMART
* [Ravichandiran] Getting Started with Google BERT
* [101ai.net] `BERT Explorer <https://www.101ai.net/text/bert>`_
* [Rothman] Transformers for Natural Language Processing
* [Tunstall] Natural Language Processing with Transformers
* [lilianweng] `The Transformer Family Version 2.0 <https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/>`_
* [Lakshmanan] Practical Machine Learning for Computer Vision
* Recent Advances and Trends in Multimodal Deep Learning
* Recommender Systems
* [Stanford] `CS224n: Natural Language Processing with Deep Learning <https://web.stanford.edu/class/cs224n/index.html>`_
* [Stanford] `CS224U - Natural Language Understanding <https://www.youtube.com/playlist?list=PLoROMvodv4rPt5D0zs3YhbWSZA8Q_DyiJ>`_
* [Stanford] `CS25 - Transformers United <https://www.youtube.com/playlist?list=PLoROMvodv4rNiJRchCzutFw5ItR_Z27CM>`_
* [Stanford] `CS330 - Deep Multi-Task and Meta-Learning <https://www.youtube.com/playlist?list=PLoROMvodv4rMIJ-TvblAIkw28Wxi27B36>`_
* `From Deep to Long Learning? <https://hazyresearch.stanford.edu/blog/2023-03-27-long-learning>`_
* [CMU] `Graham Neubig's Teaching <https://www.phontron.com/teaching.php>`_
* [Princeton] `Against Predictive Optimization <https://predictive-optimization.cs.princeton.edu/>`_
* [Github] `Must Read Papers on Pre-Training <https://github.com/thunlp/PLMpapers>`_
* `NaturalSpeech 2: Latent Diffusion Models are Natural and Zero-Shot Speech and Singing Synthesizers <https://speechresearch.github.io/naturalspeech2/>`_

******************************************************************************************
ML Papers
******************************************************************************************

* [dair-ai] `ML-Papers-Explained <https://github.com/dair-ai/ML-Papers-Explained>`_
* `Transformer models: an introduction and catalog — 2023 Edition <https://amatriain.net/blog/transformer-models-an-introduction-and-catalog-2d1e9039f376/>`_
* [Meta AI] `Teaching AI advanced mathematical reasoning <https://ai.facebook.com/blog/ai-math-theorem-proving/?utm_campaign=evergreen&utm_source=linkedin&utm_medium=organic_social&utm_content=blog>`_
* [Microsoft Research] `Why Can GPT Learn In-Context? <https://arxiv.org/pdf/2212.10559v2.pdf>`_
* [HM] `ML papers to implement <https://news.ycombinator.com/item?id=34503362>`_
* [ICLR2023] `Diffusion Models already have a Semantic Latent Space <https://arxiv.org/pdf/2210.10960.pdf>`_

******************************************************************************************
MLE Papers
******************************************************************************************

* [ACM] DNN for YouTube Recommendations
* [FB] Local Search
* [FB] Photo Search
* [FB] Recommeding items to more than a billion people
* [ICML] ScaNN
* [NeurIPS] DiskANN
* [KDD] Predicting Clicks on Ads at Facebook
* [RecSys] Recommending What Video to Watch Next
* `91% of ML Models Degrade in Time <https://www.nannyml.com/blog/91-of-ml-perfomance-degrade-in-time>`_

******************************************************************************************
MLOps
******************************************************************************************

* `The big dictionary of MLOps <https://www.hopsworks.ai/mlops-dictionary>`_

******************************************************************************************
ML Interviews
******************************************************************************************

* [Kashan] Deep Learning Interviews

******************************************************************************************
System Design General Principles
******************************************************************************************

* [Kleppmann] Designing Data-Intensive Applications
* [Alex Xu] System Design Interview - An Insiders Guide
* [Alex Xu] System Design Interview - An Insider's Guide Volume 2
* [Donne Matrin] `System Design Primer <https://github.com/donnemartin/system-design-primer>`_
* [Binh Nguyen] `Awesome Scalability <https://github.com/binhnguyennus/awesome-scalability>`_
* [Educative] `Grokking Modern System Design Interview for Engineers & Managers <https://www.educative.io/courses/grokking-modern-system-design-interview-for-engineers-managers>`_
* `A Senior Engineer's Guide to System Design Interview <https://interviewing.io/guides/system-design-interview>`_

******************************************************************************************
System Design Algorithms
******************************************************************************************

* [Gakhov] Probabilistic Data Structures and Algorithms
* [Tyler Neylon] `Introduction to Locality-Sensitive Hashing <https://tylerneylon.com/a/lsh1/lsh_post1.html>`_

******************************************************************************************
System Design Practical
******************************************************************************************

* `Build Your Own Redis with C/C++ <https://build-your-own.org/redis/>`_
* `Build Your Own Database <https://build-your-own.org/blog/20230420_byodb_done/>`_
* `The Inner Workings of Distributed Databases <https://questdb.io/blog/inner-workings-distributed-databases/>`_

******************************************************************************************
Layoffs
******************************************************************************************

* `Effective Immediately <https://github.com/Effective-Immediately/effective-immediately>`_

******************************************************************************************
Misc
******************************************************************************************
* `Fully Dynamic k-Clustering with Fast Update Time and Small Recourse <https://arxiv.org/abs/2408.01325>`_
* `Topology From The Ground Up: A Comic <https://processoveroutcome.substack.com/p/topology-from-the-ground-up?r=4irfl>`_
* `Sampling - Interesting post on LinkedIn <https://www.linkedin.com/posts/sahil0094_sampling-trainingdata-machinelearnig-activity-7043559310324285440-58h2>`_
* [Developer-Y] `CS Video Courses <https://github.com/Developer-Y/cs-video-courses>`_
* `Openintro Statistics <https://www.openintro.org/book/os/>`_
* `Demystifying Fourier analysis <https://dsego.github.io/demystifying-fourier/>`_
* `Data-oriented Programming in Python <https://www.moderndescartes.com/essays/data_oriented_python/>`_
* [CMU] `15-751 CS Theory Toolkit <https://www.youtube.com/playlist?app=desktop&list=PLm3J0oaFux3ZYpFLwwrlv_EHH9wtH6pnX>`_
* `Data Structure Sketches <https://okso.app/showcase/data-structures>`_
* [HN] `Vectors are over, hashes are the future <https://news.ycombinator.com/item?id=33123972>`_
* `Tensor Search <https://www.reddit.com/r/MachineLearning/comments/xk31n8/p_my_cofounder_and_i_quit_our_engineering_jobs_at/>`_
* `Philosophy of Mathematics - A Readinng List <https://www.logicmatters.net/2020/11/16/philosophy-of-mathematics-a-reading-list/>`_
* `The faker's guide to reading (x86) assembly language <https://www.timdbg.com/posts/fakers-guide-to-assembly/>`_
* `Learn C++ <https://www.learncpp.com/>`_
* `Introducing Austral: A Systems Language with Linear Types and Capabilities <https://borretti.me/article/introducing-austral>`_
* `A Beautiful Mathematical Reading List for 2023 <https://abakcus.com/a-beautiful-mathematical-reading-list-for-2023/>`_
* `Vector Animations With Python <https://zulko.github.io/blog/2014/09/20/vector-animations-with-python/>`_
* `Systems design 2: What we hope we know <https://apenwarr.ca/log/20230415>`_
* `Irregular Expressions <https://tavianator.com/2023/irregex.html>`_
* `The Prospect of an AI Winter <https://www.erichgrunewald.com/posts/the-prospect-of-an-ai-winter/>`_
* `When Will AI Take Your Job? <https://unchartedterritories.tomaspueyo.com/p/when-will-ai-take-your-job>`_
* `What Is Disruptive Innovation? <https://hbr.org/2015/12/what-is-disruptive-innovation>`_
* `Category Theory ∩ Machine Learning <https://github.com/bgavran/Category_Theory_Machine_Learning>`_
* `Building a Better World without Jobs <https://workforcefuturist.substack.com/p/building-a-better-world-without-jobs-video>`_
* `The Joy of Abstraction - An Introduction to Category Theory <https://johncarlosbaez.wordpress.com/2023/02/11/the-joy-of-abstraction/>`_
* `Clean Code - Horrible Performance <https://www.computerenhance.com/p/clean-code-horrible-performance>`_
* `Reverse Engineering a Mysterious UDP stream in my hotel <https://www.gkbrk.com/2016/05/hotel-music/>`_
* `Procrastinating is linked to health and career problems <https://theconversation.com/procrastinating-is-linked-to-health-and-career-problems-but-there-are-things-you-can-do-to-stop-188322>`_
* `Map of Reddit <https://anvaka.github.io/map-of-reddit/?v=2>`_
* `The Embedding Archives: Millions of Wikipedia Article Embeddings in Many Languages <https://txt.cohere.com/embedding-archives-wikipedia/>`_
* `Why Oatmeal is Cheap: Kolmogorov Complexity and Procedural Generation <https://knivesandpaintbrushes.org/projects/why-oatmeal-is-cheap/why_oatmeal_is_cheap_fdg2023.pdf>`_
* `Blog: Haskell in Production <https://serokell.io/blog/haskell-in-production>`_
* `How Does an FPGA Work? <https://learn.sparkfun.com/tutorials/how-does-an-fpga-work/all>`_
* [readthedocs.io] `Sphinx Admonitions <https://sphinx-immaterial.readthedocs.io/en/latest/admonitions.html>`_
